<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Che Liu</title>
  <link rel="stylesheet" href="assets/css/style.css?v=1.1">
  <meta name="description" content="Academic homepage of Che Liu">
</head>
<body>
  <section class="hero">
    <button class="theme-toggle" id="theme-toggle" title="Toggle dark/light mode">
      <svg class="sun-icon" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364-6.364l-.707.707M6.343 17.657l-.707.707M16.95 16.95l.707.707M7.05 7.05l.707.707M12 8a4 4 0 100 8 4 4 0 000-8z" />
      </svg>
      <svg class="moon-icon" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z" />
      </svg>
    </button>
    <div class="container">
      <h1>Hello, I'm Che Liu</h1>
      <div class="hero-grid">
        <div class="hero-text">
          <p>I am a final-year PhD student at <a href="https://www.imperial.ac.uk/" target="_blank" rel="noopener">Imperial College London</a>, supervised by <a href="https://profiles.imperial.ac.uk/r.arcucci" target="_blank" rel="noopener">Prof. Rossella Arcucci</a> and <a href="https://profiles.imperial.ac.uk/w.bai" target="_blank" rel="noopener">Prof. Wenjia Bai</a>. I was a visiting student at <a href="https://www.tum.de/" target="_blank" rel="noopener">Technical University of Munich (TUM)</a>, advised by <a href="https://www.professoren.tum.de/rueckert-daniel" target="_blank" rel="noopener">Prof. Dr. Daniel Rückert</a>.</p>
          <p>I have also conducted research at <a href="https://www.x-humanoid.com/" target="_blank" rel="noopener">X-Humanoid</a> on Embodied Understanding and Generation, <a href="https://stepfun.ai/research" target="_blank" rel="noopener">StepFun</a> on Omni-modal learning, <a href="https://damo.alibaba.com/research-areas?language=en" target="_blank" rel="noopener">DAMO Academy</a> on Visual Pre-training, and <a href="https://www.astrazeneca.co.uk/" target="_blank" rel="noopener">AstraZeneca Cambridge</a> on VLM.</p>
          <p class="text-red">I am actively seeking a full-time research position based in the US, UK, or China, with a focus on multimodal learning, unified visual understanding and generation, and their applications.</p>
          <p>
            Email: che.liu21 [at] imperial.ac.uk
          </p>
          <div class="button-row">
            <a class="btn" href="assets/files/cv.pdf">CV</a>
            <a class="btn" href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
            <a class="btn" href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">LinkedIn</a>
          </div>
        </div>
        <div class="portrait">
          <img src="assets/img/profile.png" alt="Che Liu portrait" class="profile-pic">
        </div>
      </div>

      <!-- <h4 class="title" style="margin-top: 26px; margin-bottom: 6px; font-size: 20px;">News</h4>
      <ul class="news-list">
        <li><span class="news-date">Apr 2025</span> Visiting Researcher (Embodied VLM) @ X-humanoid.</li>
        <li><span class="news-date">May 2025</span> Visiting Researcher (OmniLLM) @ StepFun.</li>
        <li><span class="news-date">Nov 2024</span> Research Intern (Unified Multimodal Vision Pretraining) @ DAMO Academy.</li>
        <li><span class="news-date">2024</span> Invited talks: Synthetic Data for Medical Multimodal Learning (AstraZeneca), Multimodal Medical AI (Stanford MedAI), ECG Multimodal Learning (PKU).</li>
      </ul> -->
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="publications">
    <div class="container">
      <h2>Selected Publications</h2>
      <p class="pub-meta" style="margin-bottom: 14px;">† - Equal contribution, ‡ - Supervision. More on <a href="https://scholar.google.co.uk/citations?user=HED_458AAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a>.</p>

      <div class="pub-tabs" role="tablist">
        <button class="tab-btn active" data-target="tab-omni" aria-selected="true">Omni-modal Learning (Audio • Visual • Language)</button>
        <button class="tab-btn" data-target="tab-medical" aria-selected="false">Multimodal Learning in Healthcare</button>
      </div>

      <div class="pub-panel active" id="tab-omni" role="tabpanel">
      <div class="pub-grid">
        <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Pelican-VL"> -->
            <div>
              <h3 class="pub-title"><a href="https://dl.acm.org/doi/pdf/10.1145/3746027.3754752" target="_blank" rel="noopener">Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, X-Humanoid</p>
              <p class="pub-meta"><strong>Technical Report (X-humanoid)</strong>, 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Nexus-O"> -->
          <div>
              <h3 class="pub-title"><a href="https://dl.acm.org/doi/pdf/10.1145/3746027.3754752" target="_blank" rel="noopener">Nexus-O: An Omni-Perceptive And Interactive Model for Language, Audio, And Vision</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>ACM Multimedia</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Step-Audio 2"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2507.16632" target="_blank" rel="noopener">Step-Audio 2</a></h3>
              <p class="pub-authors">StepFun Audio Team</p>
              <p class="pub-meta"><strong>Technical Report (StepFun)</strong></p>
          </div>
        </div>
        <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="SRPO"> -->
          <div>
              <h3 class="pub-title"><a href="https://srpo.pages.dev/" target="_blank" rel="noopener">SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning</a></h3>
              <p class="pub-authors">Z. Wan, <strong>C. Liu</strong>, S. Yan, et al.</p>
              <p class="pub-meta"><strong>NeurIPS</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Ariadne"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2511.00710" target="_blank" rel="noopener">Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries</a></h3>
              <p class="pub-authors">M. Shen, Z. Tu, <strong>C. Liu‡</strong>, et al.</p>
              <p class="pub-meta"><strong>Technical Report</strong></p>
          </div>
        </div>
        <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Emergent Hierarchical Reasoning"> -->
          <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.03646" target="_blank" rel="noopener">Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</a></h3>
              <p class="pub-authors">H. Wang, <strong>C. Liu</strong>, W. Chen, et al.</p>
              <p class="pub-meta"><strong>Technical Report</strong></p>
            </div>
          </div>
        </div>
      </div>

      <div class="pub-panel" id="tab-medical" role="tabpanel">
        <div class="pub-grid">
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="DINOv3"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.06467" target="_blank" rel="noopener">Does DINOv3 Set a New Medical Vision Standard?</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>Technical Report</strong></p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="NOVA"> -->
            <div>
              <h3 class="pub-title"><a href="https://neurips.cc/virtual/2025/loc/san-diego/poster/121770" target="_blank" rel="noopener">NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI</a></h3>
              <p class="pub-authors">Cosmin I. Bercea, <strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>NeurIPS Dataset and Benchmark Track</strong> 2025 <strong>(Oral)</strong></p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Beyond distillation"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2505.17952" target="_blank" rel="noopener">Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>NeurIPS GenAI4Health Workshop</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="T3D"> -->
            <div>
              <h3 class="pub-title"><a href="https://openaccess.thecvf.com/content/ICCV2025W/VLM3D/html/Liu_T3D_Advancing_3D_Medical_Vision-Language_Pre-training_by_Learning_Multi-View_Visual_ICCVW_2025_paper.html" target="_blank" rel="noopener">T3D: Advancing 3D Medical Vision-Language Pre-training by Learning Multi-View Visual Consistency</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>ICCV VLM3D Workshop</strong> 2025 <strong>(Oral)</strong></p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="M3Ret"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.01360" target="_blank" rel="noopener">M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>Technical Report (DAMO Academy)</strong>, 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Knowledge-enhanced"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-emnlp.385/" target="_blank" rel="noopener">Knowledge-Enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>EMNLP Findings</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="SuPreME"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-emnlp.633/" target="_blank" rel="noopener">SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</a></h3>
              <p class="pub-authors">M. Cai, J. Jiang, W. Huang, <strong>C. Liu‡</strong>, R. Arcucci</p>
              <p class="pub-meta"><strong>EMNLP Findings</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="NEJM AI"> -->
            <div>
              <h3 class="pub-title"><a href="https://ai.nejm.org/doi/10.1056/AIoa2401033" target="_blank" rel="noopener">An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains</a></h3>
              <p class="pub-authors">J. Li, A. Aguirre, J. Moura, <strong>C. Liu</strong>, L. Zhong, C. Sun, G. Clifford, B. Westover, S. Hong</p>
              <p class="pub-meta"><strong>NEJM AI</strong> 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Synthetic Data"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.843/" target="_blank" rel="noopener">Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>ACL Findings</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Argus"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.845/" target="_blank" rel="noopener">Argus: Benchmarking and Enhancing Vision-Language Models for 3D Radiology Report Generation</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>ACL Findings</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="MEIT"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.749/" target="_blank" rel="noopener">MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation</a></h3>
              <p class="pub-authors">Z. Wan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta"><strong>ACL Findings</strong> 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="MedVLM-R1"> -->
            <div>
              <h3 class="pub-title"><a href="https://link.springer.com/chapter/10.1007/978-3-032-04981-0_32" target="_blank" rel="noopener">MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning</a></h3>
              <p class="pub-authors">J. Pan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta"><strong>MICCAI</strong> 2025 (Early Accept)</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Zero-Shot ECG"> -->
            <div>
              <h3 class="pub-title"><a href="https://icml.cc/virtual/2024/poster/33716" target="_blank" rel="noopener">Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>ICML</strong> 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="G2D"> -->
            <div>
              <h3 class="pub-title"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/1ac14e44228aeadabb3c934822c1212a-Abstract-Conference.html" target="_blank" rel="noopener">G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>NeurIPS</strong> 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="IMITATE"> -->
            <div>
              <h3 class="pub-title"><a href="https://ieeexplore.ieee.org/abstract/document/10646593/" target="_blank" rel="noopener">IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>IEEE Transactions on Medical Imaging</strong> 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Med-UniC"> -->
            <div>
              <h3 class="pub-title"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html" target="_blank" rel="noopener">Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias</a></h3>
              <p class="pub-authors">Z. Wan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta"><strong>NeurIPS</strong> 2023</p>
          </div>
        </div>
        <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="M-FLAG"> -->
          <div>
              <h3 class="pub-title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-43907-0_61" target="_blank" rel="noopener">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>MICCAI</strong> 2023</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Spectral Cross-Domain"> -->
            <div>
              <h3 class="pub-title"><a href="https://ieeexplore.ieee.org/abstract/document/10328660" target="_blank" rel="noopener">Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta"><strong>IEEE TNNLS</strong> 2023</p>
          </div>
        </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Frozen Language Model"> -->
          <div>
              <h3 class="pub-title"><a href="https://proceedings.mlr.press/v227/li24a.html" target="_blank" rel="noopener">Frozen Language Model Helps ECG Zero-Shot Learning</a></h3>
              <p class="pub-authors">J. Li†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta"><strong>MIDL</strong> 2023 <strong>(Oral)</strong></p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="experience">
    <div class="container">
      <h2>Experiences</h2>
      <table class="exp-table">
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>X-humanoid</b>, Remote<br>
            Visiting Researcher (Embodied VLM) — Apr 2025 to Present<br>
            Leading VLM post-training (SFT + self-evolving RL) on large-scale video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/humanoid.png" alt="X-humanoid logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>StepFun</b>, Remote<br>
            Visiting Researcher (OmniLLM) — May 2025 to Present<br>
            Audio-visual-language SFT/RL and omni-capability benchmarks.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/stepfun.png" alt="StepFun logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>DAMO Academy</b>, Beijing<br>
            Research Intern — Nov 2024 to Apr 2025<br>
            Unified multimodal vision pretraining across 2D/3D/video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/damo.jpg" alt="DAMO Academy logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>AstraZeneca</b>, Cambridge, UK<br>
            Research Intern (Vision-Language Models) — Jul 2024 to Sep 2024<br>
            Synthetic-data VLP; showed synthetic pretraining can beat real-data baselines.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/astrazeneca.png" alt="AstraZeneca logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="education">
    <div class="container">
      <h2>Education</h2>
      <table class="edu-table">
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Imperial College London</b>, UK<br>
            Ph.D. in Multimodal Learning — Feb 2022 to 2026 (expected)<br>
            Supervisors: <a href="https://profiles.imperial.ac.uk/r.arcucci" target="_blank" rel="noopener">Rossella Arcucci</a>, <a href="https://www.doc.ic.ac.uk/~wbai/web/" target="_blank" rel="noopener">Wenjia Bai</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/imperial.png" alt="Imperial College London logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Technical University of Munich</b>, Germany<br>
            Visiting Ph.D. — Apr 2024 to Jun 2024<br>
            Supervisor: <a href="https://scholar.google.com/citations?user=H0O0WnQAAAAJ&hl=en" target="_blank" rel="noopener">Daniel Rückert</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/tum.png" alt="Technical University of Munich logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Swansea University</b>, UK<br>
            M.Sc. (Distinction) in Computational Mechanics — Sep 2019 to Sep 2021.<br>
            Supervisor: <a href="https://scholar.google.com/citations?hl=en&user=_P98CR0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Dunhui Xiao</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/swansea.png" alt="Swansea University logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Shanghai University of Engineering and Science</b>, China<br>
            B.Sc. in Automotive Engineering — Sep 2016 to Jul 2018.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/sues.png" alt="Shanghai University of Engineering and Science logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <footer class="footer" id="contact">
    <div class="container">
      <div>
        <script>
          const user = 'che.liu21';
          const domain = 'imperial.ac.uk';
          document.write(`<a href="mailto:${user}@${domain}">Email</a>`);
        </script>
        <noscript>Email</noscript> ·
        <a href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">LinkedIn</a> ·
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
      </div>
      <div style="margin-top: 8px;">
        © 2025 Che Liu. Last updated: Dec 29, 2025.
      </div>
    </div>
  </footer>

  <script>
    const tabButtons = document.querySelectorAll('.tab-btn');
    const tabPanels = document.querySelectorAll('.pub-panel');

    function activateTab(button) {
      const targetId = button.dataset.target;
      tabButtons.forEach((btn) => btn.classList.toggle('active', btn === button));
      tabButtons.forEach((btn) => btn.setAttribute('aria-selected', btn === button));
      tabPanels.forEach((panel) => panel.classList.toggle('active', panel.id === targetId));
    }

    tabButtons.forEach((btn) => {
      btn.addEventListener('mouseenter', () => activateTab(btn));
      btn.addEventListener('focus', () => activateTab(btn));
    });

    // Theme Toggle Logic
    const themeToggle = document.getElementById('theme-toggle');
    const htmlElement = document.documentElement;
    
    // Check for saved theme preference
    const savedTheme = localStorage.getItem('theme');
    if (savedTheme) {
      htmlElement.setAttribute('data-theme', savedTheme);
    }

    themeToggle.addEventListener('click', () => {
      const currentTheme = htmlElement.getAttribute('data-theme');
      let newTheme;
      
      if (currentTheme) {
        newTheme = currentTheme === 'dark' ? 'light' : 'dark';
      } else {
        // If no manual theme set, check system preference
        const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
        newTheme = prefersDark ? 'light' : 'dark';
      }
      
      htmlElement.setAttribute('data-theme', newTheme);
      localStorage.setItem('theme', newTheme);
    });
  </script>
</body>
</html>

