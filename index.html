<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Che Liu</title>
  <link rel="stylesheet" href="assets/css/style.css?v=1.1">
  <meta name="description" content="Academic homepage of Che Liu">
</head>
<body>
  <section class="hero">
    <div class="container">
      <h1>Hello, I'm Che Liu</h1>
      <div class="hero-grid">
        <div class="hero-text">
          <p>I am a researcher focusing on multimodal learning across 2D/3D vision, video, audio, language, and time-series data. I work on unified models for perception and generation, combining large-scale pretraining and RL for structured cross-modal reasoning.</p>
          <p>
            Email: che.liu21@imperial.ac.uk
          </p>
          <div class="button-row">
            <a class="btn" href="assets/files/cv.pdf">CV</a>
            <a class="btn" href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
            <a class="btn" href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">LinkedIn</a>
          </div>
        </div>
        <div class="portrait">
          <img src="assets/img/profile.png" alt="Che Liu portrait" class="profile-pic">
        </div>
      </div>

      <!-- <h4 class="title" style="margin-top: 26px; margin-bottom: 6px; font-size: 20px;">News</h4>
      <ul class="news-list">
        <li><span class="news-date">Apr 2025</span> Visiting Researcher (Embodied VLM) @ X-humanoid.</li>
        <li><span class="news-date">May 2025</span> Visiting Researcher (OmniLLM) @ StepFun.</li>
        <li><span class="news-date">Nov 2024</span> Research Intern (Unified Multimodal Vision Pretraining) @ DAMO Academy.</li>
        <li><span class="news-date">2024</span> Invited talks: Synthetic Data for Medical Multimodal Learning (AstraZeneca), Multimodal Medical AI (Stanford MedAI), ECG Multimodal Learning (PKU).</li>
      </ul> -->
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="publications">
    <div class="container">
      <h2>Selected Publications</h2>
      <p class="pub-meta" style="margin-bottom: 14px;">† - Equal contribution, ‡ - Supervision. More on <a href="https://scholar.google.co.uk/citations?user=HED_458AAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a>.</p>

      <div class="pub-tabs" role="tablist">
        <button class="tab-btn active" data-target="tab-omni" aria-selected="true">Omni-modal Learning (Audio • Visual • Language)</button>
        <button class="tab-btn" data-target="tab-medical" aria-selected="false">Multimodal Learning in Healthcare</button>
      </div>

      <div class="pub-panel active" id="tab-omni" role="tabpanel">
        <div class="pub-grid">
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Pelican-VL"> -->
            <div>
              <h3 class="pub-title"><a href="https://dl.acm.org/doi/pdf/10.1145/3746027.3754752" target="_blank" rel="noopener">Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, X-Humanoid</p>
              <p class="pub-meta">Technical Report, 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Nexus-O"> -->
            <div>
              <h3 class="pub-title"><a href="https://dl.acm.org/doi/pdf/10.1145/3746027.3754752" target="_blank" rel="noopener">Nexus-O: An Omni-Perceptive And Interactive Model for Language, Audio, And Vision</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">ACM Multimedia 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Step-Audio 2"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2507.16632" target="_blank" rel="noopener">Step-Audio 2</a></h3>
              <p class="pub-authors">StepFun Audio Team</p>
              <p class="pub-meta">Technical Report</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="SRPO"> -->
            <div>
              <h3 class="pub-title"><a href="https://srpo.pages.dev/" target="_blank" rel="noopener">SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning</a></h3>
              <p class="pub-authors">Z. Wan, <strong>C. Liu</strong>, S. Yan, et al.</p>
              <p class="pub-meta">NeurIPS 2025</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Ariadne"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2511.00710" target="_blank" rel="noopener">Ariadne: A Controllable Framework for Probing and Extending VLM Reasoning Boundaries</a></h3>
              <p class="pub-authors">M. Shen, Z. Tu, <strong>C. Liu‡</strong>, et al.</p>
              <p class="pub-meta">Technical Report</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Emergent Hierarchical Reasoning"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.03646" target="_blank" rel="noopener">Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning</a></h3>
              <p class="pub-authors">H. Wang, <strong>C. Liu</strong>, W. Chen, et al.</p>
              <p class="pub-meta">Technical Report</p>
            </div>
          </div>
        </div>
      </div>

      <div class="pub-panel" id="tab-medical" role="tabpanel">
        <div class="pub-grid">
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="DINOv3"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.06467" target="_blank" rel="noopener">Does DINOv3 Set a New Medical Vision Standard?</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">Technical Report</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Beyond distillation"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2505.17952" target="_blank" rel="noopener">Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">NeurIPS 2025 GenAI4Health Workshop</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="T3D"> -->
            <div>
              <h3 class="pub-title"><a href="https://openaccess.thecvf.com/content/ICCV2025W/VLM3D/html/Liu_T3D_Advancing_3D_Medical_Vision-Language_Pre-training_by_Learning_Multi-View_Visual_ICCVW_2025_paper.html" target="_blank" rel="noopener">T3D: Advancing 3D Medical Vision-Language Pre-training by Learning Multi-View Visual Consistency</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">ICCV 2025 VLM3D Workshop (Oral)</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="M3Ret"> -->
            <div>
              <h3 class="pub-title"><a href="https://arxiv.org/abs/2509.01360" target="_blank" rel="noopener">M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">Technical Report (DAMO Academy)</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Knowledge-enhanced"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-emnlp.385/" target="_blank" rel="noopener">Knowledge-Enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">EMNLP 2025 Findings</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="SuPreME"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-emnlp.633/" target="_blank" rel="noopener">SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</a></h3>
              <p class="pub-authors">M. Cai, J. Jiang, W. Huang, <strong>C. Liu</strong> (Corresponding), R. Arcucci</p>
              <p class="pub-meta">EMNLP 2025 Findings</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="NEJM AI"> -->
            <div>
              <h3 class="pub-title"><a href="https://ai.nejm.org/doi/10.1056/AIoa2401033" target="_blank" rel="noopener">An Electrocardiogram Foundation Model Built on over 10 Million Recordings with External Evaluation across Multiple Domains</a></h3>
              <p class="pub-authors">J. Li, A. Aguirre, J. Moura, <strong>C. Liu</strong>, L. Zhong, C. Sun, G. Clifford, B. Westover, S. Hong</p>
              <p class="pub-meta">NEJM AI 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Synthetic Data"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.843/" target="_blank" rel="noopener">Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">ACL 2025 Findings</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Argus"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.845/" target="_blank" rel="noopener">Argus: Benchmarking and Enhancing Vision-Language Models for 3D Radiology Report Generation</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">ACL 2025 Findings</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="MEIT"> -->
            <div>
              <h3 class="pub-title"><a href="https://aclanthology.org/2025.findings-acl.749/" target="_blank" rel="noopener">MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation</a></h3>
              <p class="pub-authors">Z. Wan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta">ACL 2025 Findings</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="MedVLM-R1"> -->
            <div>
              <h3 class="pub-title"><a href="https://link.springer.com/chapter/10.1007/978-3-032-04981-0_32" target="_blank" rel="noopener">MedVLM-R1: Incentivizing Medical Reasoning Capability of Vision-Language Models (VLMs) via Reinforcement Learning</a></h3>
              <p class="pub-authors">J. Pan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta">MICCAI 2025 (Early Accept)</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Zero-Shot ECG"> -->
            <div>
              <h3 class="pub-title"><a href="https://icml.cc/virtual/2024/poster/33716" target="_blank" rel="noopener">Zero-Shot ECG Classification with Multimodal Learning and Test-time Clinical Knowledge Enhancement</a></h3>
              <p class="pub-authors"><strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta">ICML 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="G2D"> -->
            <div>
              <h3 class="pub-title"><a href="https://proceedings.neurips.cc/paper_files/paper/2024/hash/1ac14e44228aeadabb3c934822c1212a-Abstract-Conference.html" target="_blank" rel="noopener">G2D: From Global to Dense Radiography Representation Learning via Vision-Language Pre-training</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">NeurIPS 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="IMITATE"> -->
            <div>
              <h3 class="pub-title"><a href="https://ieeexplore.ieee.org/abstract/document/10646593/" target="_blank" rel="noopener">IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">IEEE Transactions on Medical Imaging 2024</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Med-UniC"> -->
            <div>
              <h3 class="pub-title"><a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/af38fb8e90d586f209235c94119ba193-Abstract-Conference.html" target="_blank" rel="noopener">Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias</a></h3>
              <p class="pub-authors">Z. Wan†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta">NeurIPS 2023</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="M-FLAG"> -->
            <div>
              <h3 class="pub-title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-43907-0_61" target="_blank" rel="noopener">M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">MICCAI 2023</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Spectral Cross-Domain"> -->
            <div>
              <h3 class="pub-title"><a href="https://ieeexplore.ieee.org/abstract/document/10328660" target="_blank" rel="noopener">Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement</a></h3>
              <p class="pub-authors"><strong>C. Liu</strong>, et al.</p>
              <p class="pub-meta">IEEE TNNLS 2023</p>
            </div>
          </div>
          <div class="pub-card">
            <!-- <img class="pub-thumb" src="assets/img/profile.png" alt="Frozen Language Model"> -->
            <div>
              <h3 class="pub-title"><a href="https://proceedings.mlr.press/v227/li24a.html" target="_blank" rel="noopener">Frozen Language Model Helps ECG Zero-Shot Learning</a></h3>
              <p class="pub-authors">J. Li†, <strong>C. Liu†</strong>, et al.</p>
              <p class="pub-meta">MIDL 2023 (Oral)</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="experience">
    <div class="container">
      <h2>Experiences</h2>
      <table class="exp-table">
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>X-humanoid</b>, Remote<br>
            Visiting Researcher (Embodied VLM) — Apr 2025 to Present<br>
            Leading VLM post-training (SFT + self-evolving RL) on large-scale video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/humanoid.png" alt="X-humanoid logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>StepFun</b>, Remote<br>
            Visiting Researcher (OmniLLM) — May 2025 to Present<br>
            Audio-visual-language SFT/RL and omni-capability benchmarks.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/stepfun.png" alt="StepFun logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>DAMO Academy</b>, Beijing<br>
            Research Intern — Nov 2024 to Apr 2025<br>
            Unified multimodal vision pretraining across 2D/3D/video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/damo.jpg" alt="DAMO Academy logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>AstraZeneca</b>, Cambridge, UK<br>
            Research Intern (Vision-Language Models) — Jul 2024 to Sep 2024<br>
            Synthetic-data VLP; showed synthetic pretraining can beat real-data baselines.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/astrazeneca.png" alt="AstraZeneca logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="education">
    <div class="container">
      <h2>Education</h2>
      <table class="edu-table">
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Imperial College London</b>, UK<br>
            Ph.D. in Multimodal Learning — Feb 2022 to 2026 (expected)<br>
            Supervisors: <a href="https://profiles.imperial.ac.uk/r.arcucci" target="_blank" rel="noopener">Rossella Arcucci</a>, <a href="https://www.doc.ic.ac.uk/~wbai/web/" target="_blank" rel="noopener">Wenjia Bai</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/imperial.png" alt="Imperial College London logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Technical University of Munich</b>, Germany<br>
            Visiting Ph.D. — Apr 2024 to Jun 2024<br>
            Supervisor: <a href="https://scholar.google.com/citations?user=H0O0WnQAAAAJ&hl=en" target="_blank" rel="noopener">Daniel Rückert</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/tum.png" alt="Technical University of Munich logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Swansea University</b>, UK<br>
            M.Sc. (Distinction) in Computational Mechanics — Sep 2019 to Sep 2021.<br>
            Supervisor: <a href="https://scholar.google.com/citations?hl=en&user=_P98CR0AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Dunhui Xiao</a>.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/swansea.png" alt="Swansea University logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Shanghai University of Engineering and Science</b>, China<br>
            B.Sc. in Automotive Engineering — Sep 2016 to Jul 2018.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/sues.png" alt="Shanghai University of Engineering and Science logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <footer class="footer" id="contact">
    <div class="container">
      <div>
        <a href="mailto:che.liu21@imperial.ac.uk">Email</a> ·
        <a href="https://github.com/cheliu-computation" target="_blank" rel="noopener">GitHub</a> ·
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
      </div>
      <div style="margin-top: 8px;">
        © 2025 Che Liu. Last updated: Dec 19, 2025.
      </div>
    </div>
  </footer>

  <script>
    const tabButtons = document.querySelectorAll('.tab-btn');
    const tabPanels = document.querySelectorAll('.pub-panel');

    function activateTab(button) {
      const targetId = button.dataset.target;
      tabButtons.forEach((btn) => btn.classList.toggle('active', btn === button));
      tabButtons.forEach((btn) => btn.setAttribute('aria-selected', btn === button));
      tabPanels.forEach((panel) => panel.classList.toggle('active', panel.id === targetId));
    }

    tabButtons.forEach((btn) => {
      btn.addEventListener('mouseenter', () => activateTab(btn));
      btn.addEventListener('focus', () => activateTab(btn));
    });
  </script>
</body>
</html>

