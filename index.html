<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Che Liu | Academic Homepage</title>
  <link rel="stylesheet" href="assets/css/style.css">
  <meta name="description" content="Academic homepage of Che Liu">
</head>
<body>
  <header>
    <div class="topbar">
      <div class="brand">Che Liu</div>
      <div class="nav-links">
        <a href="#about">About</a>
        <a href="#news">News</a>
        <a href="#publications">Publications</a>
        <a href="#experience">Experience</a>
        <a href="#education">Education</a>
        <a href="#contact">Contact</a>
      </div>
    </div>
  </header>

  <main>
    <section id="about" class="intro">
      <div>
        <h1>Che Liu</h1>
        <p class="tagline">Researcher in multimodal learning (vision, audio, language)</p>
        <p class="summary">
          I focus on multimodal understanding and unified models for perception and generation across 2D/3D vision, video, audio, language, and time-series data. My work combines large-scale pre-training with reinforcement learning to enable structured cross-modal reasoning—aiming for scalable, generalizable systems for complex real-world tasks.
        </p>
        <div class="link-row">
          <a href="mailto:che.liu21@imperial.ac.uk">che.liu21@imperial.ac.uk</a>
          <span>·</span>
          <a href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
          <span>·</span>
          <a href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">LinkedIn</a>
          <span>·</span>
          <a href="https://github.com/cheliu-computation" target="_blank" rel="noopener">GitHub</a>
          <span>·</span>
          <a href="assets/files/cv.pdf">CV</a>
        </div>
      </div>
      <div class="intro-photo">
        <img src="assets/img/profile.png" alt="Che Liu portrait" class="profile-pic" style="object-fit: cover; width: 160px; height: 160px; border-radius: 12px; border: 1px solid var(--border);">
      </div>
    </section>

    <section id="news">
      <h2>News</h2>
      <div class="card">
        <ul class="item-list">
          <li><span class="date">Apr 2025</span> Visiting Researcher (Embodied VLM) @ X-humanoid.</li>
          <li><span class="date">May 2025</span> Visiting Researcher (OmniLLM) @ StepFun.</li>
          <li><span class="date">Nov 2024</span> Research Intern (Unified Multimodal Vision Pretraining) @ DAMO Academy.</li>
          <li><span class="date">2024</span> Invited talks on Synthetic Data for Medical Multimodal Learning (AstraZeneca), Multimodal Medical AI (Stanford MedAI), ECG Multimodal Learning (PKU).</li>
        </ul>
      </div>
    </section>

    <section id="publications">
      <h2>Selected Publications</h2>
      <div class="card">
        <div class="pub">
          <div class="pub-title">Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</div>
          <div class="pub-authors"><strong>Che Liu</strong>, X-Humanoid</div>
          <div class="pub-meta">Technical Report, 2025</div>
        </div>
        <div class="pub">
          <div class="pub-title">Nexus-O: An Omni-Perceptive and Interactive Model for Language, Audio, and Vision</div>
          <div class="pub-authors"><strong>Che Liu</strong>, et al.</div>
          <div class="pub-meta">ACM Multimedia 2025</div>
        </div>
        <div class="pub">
          <div class="pub-title">Unified Visual Self-Supervised Pre-training Across Video, 2D, and 3D Vision</div>
          <div class="pub-authors"><strong>Che Liu</strong>, DAMO Academy</div>
          <div class="pub-meta">Technical Report</div>
        </div>
        <div class="pub">
          <div class="pub-title">Investigating Vision-Language Model Architectures for 3D Volume Understanding</div>
          <div class="pub-authors"><strong>Che Liu</strong>, et al.</div>
          <div class="pub-meta">ACL 2025 Findings</div>
        </div>
        <div class="pub">
          <div class="pub-title">Multimodal Time-Series Learning with Test-Time Enhancement</div>
          <div class="pub-authors"><strong>Che Liu</strong>, et al.</div>
          <div class="pub-meta">ICML 2024</div>
        </div>
      </div>
    </section>

    <section id="experience">
      <h2>Experience</h2>
      <div class="grid-two">
        <div class="card">
          <div class="pub-title">Visiting Researcher — Embodied VLM, X-humanoid</div>
          <div class="pub-authors">Remote · Apr 2025 – Present</div>
          <div class="pub-meta">Leading VLM post-training with SFT + self-evolving RL on large-scale video data (3B–72B).</div>
        </div>
        <div class="card">
          <div class="pub-title">Visiting Researcher — OmniLLM, StepFun</div>
          <div class="pub-authors">Remote · May 2025 – Present</div>
          <div class="pub-meta">Audio-visual-language SFT/RL, data curation, and omni-capability benchmarks.</div>
        </div>
        <div class="card">
          <div class="pub-title">Research Intern — Unified Multimodal Vision Pretraining, DAMO Academy</div>
          <div class="pub-authors">Beijing · Nov 2024 – Apr 2025</div>
          <div class="pub-meta">Unified vision pretraining across 2D/3D/video modalities.</div>
        </div>
        <div class="card">
          <div class="pub-title">Research Intern — Vision-Language Models, AstraZeneca</div>
          <div class="pub-authors">Cambridge, UK · Jul 2024 – Sep 2024</div>
          <div class="pub-meta">Synthetic data for VLP; demonstrated synthetic pretraining beating SOTA real-data baselines.</div>
        </div>
      </div>
    </section>

    <section id="education">
      <h2>Education</h2>
      <div class="grid-two">
        <div class="card">
          <div class="pub-title">Ph.D. in Multimodal Learning</div>
          <div class="pub-authors">Imperial College London · Feb 2022 – 2026 (expected)</div>
          <div class="pub-meta">Supervisors: Rossella Arcucci, Wenjia Bai</div>
        </div>
        <div class="card">
          <div class="pub-title">Visiting Ph.D.</div>
          <div class="pub-authors">Technical University of Munich · Apr 2024 – Jun 2024</div>
          <div class="pub-meta">Supervisor: Daniel Rückert</div>
        </div>
        <div class="card">
          <div class="pub-title">M.Sc. (Distinction), Computational Mechanics</div>
          <div class="pub-authors">Swansea University · Sep 2019 – Sep 2021</div>
          <div class="pub-meta">Formerly Erasmus Mundus Program</div>
        </div>
        <div class="card">
          <div class="pub-title">B.Sc., Automotive Engineering</div>
          <div class="pub-authors">Shanghai University of Engineering and Science · Sep 2016 – Jul 2018</div>
        </div>
      </div>
    </section>

    <section id="contact">
      <h2>Contact</h2>
      <div class="card">
        <div>Email: <a href="mailto:che.liu21@imperial.ac.uk">che.liu21@imperial.ac.uk</a></div>
        <div>Google Scholar: <a href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Profile</a></div>
        <div>LinkedIn: <a href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">Profile</a></div>
        <div>GitHub: <a href="https://github.com/cheliu-computation" target="_blank" rel="noopener">cheliu-computation</a></div>
        <div>CV: <a href="assets/files/cv.pdf">Download</a></div>
      </div>
    </section>

    <div class="footer">
      © 2025 Che Liu. Built with HTML/CSS and deployed on GitHub Pages.
    </div>
  </main>
</body>
</html>

