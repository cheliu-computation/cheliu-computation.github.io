<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Che Liu</title>
  <link rel="stylesheet" href="assets/css/style.css">
  <meta name="description" content="Academic homepage of Che Liu">
</head>
<body>
  <section class="hero">
    <div class="container">
      <h1>Hello, I'm Che Liu</h1>
      <div class="hero-grid">
        <div class="hero-text">
          <p>I am a researcher focusing on multimodal learning across 2D/3D vision, video, audio, language, and time-series data. I work on unified models for perception and generation, combining large-scale pretraining and RL for structured cross-modal reasoning.</p>
          <p>
            Email: che.liu21@imperial.ac.uk
          </p>
          <div class="button-row">
            <a class="btn" href="assets/files/cv.pdf">CV</a>
            <a class="btn" href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
            <a class="btn" href="https://github.com/cheliu-computation" target="_blank" rel="noopener">GitHub</a>
            <a class="btn" href="https://www.linkedin.com/in/che-liu-32abcs7b1/" target="_blank" rel="noopener">LinkedIn</a>
          </div>
        </div>
        <div class="portrait">
          <img src="assets/img/profile.png" alt="Che Liu portrait" class="profile-pic">
        </div>
      </div>

      <h4 class="title" style="margin-top: 26px; margin-bottom: 6px; font-size: 20px;">News</h4>
      <ul class="news-list">
        <li><span class="news-date">Apr 2025</span> Visiting Researcher (Embodied VLM) @ X-humanoid.</li>
        <li><span class="news-date">May 2025</span> Visiting Researcher (OmniLLM) @ StepFun.</li>
        <li><span class="news-date">Nov 2024</span> Research Intern (Unified Multimodal Vision Pretraining) @ DAMO Academy.</li>
        <li><span class="news-date">2024</span> Invited talks: Synthetic Data for Medical Multimodal Learning (AstraZeneca), Multimodal Medical AI (Stanford MedAI), ECG Multimodal Learning (PKU).</li>
      </ul>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="publications">
    <div class="container">
      <h2>Selected Publications</h2>
      <p class="pub-meta" style="margin-bottom: 14px;">* indicates equal contribution. More on Google Scholar.</p>
      <div class="pub-grid">
        <div class="pub-card">
          <img class="pub-thumb" src="assets/img/profile.png" alt="Pelican-VL">
          <div>
            <h3 class="pub-title">Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</h3>
            <p class="pub-authors"><strong>Che Liu</strong>, X-humanoid</p>
            <p class="pub-meta">Technical Report, 2025</p>
            <p class="pub-desc">Embodied VLM with unified capabilities across perception and action.</p>
          </div>
        </div>
        <div class="pub-card">
          <img class="pub-thumb" src="assets/img/profile.png" alt="Nexus-O">
          <div>
            <h3 class="pub-title">Nexus-O: An Omni-Perceptive and Interactive Model for Language, Audio, and Vision</h3>
            <p class="pub-authors"><strong>Che Liu</strong>, et al.</p>
            <p class="pub-meta">ACM Multimedia 2025</p>
            <p class="pub-desc">Omni-modal understanding and interaction across audio-visual-language streams.</p>
          </div>
        </div>
        <div class="pub-card">
          <img class="pub-thumb" src="assets/img/profile.png" alt="Unified Visual Pretraining">
          <div>
            <h3 class="pub-title">Unified Visual Self-Supervised Pre-training Across Video, 2D, and 3D Vision</h3>
            <p class="pub-authors"><strong>Che Liu</strong>, DAMO Academy</p>
            <p class="pub-meta">Technical Report</p>
            <p class="pub-desc">A single framework for 2D/3D/video self-supervised representation learning.</p>
          </div>
        </div>
        <div class="pub-card">
          <img class="pub-thumb" src="assets/img/profile.png" alt="3D VLM">
          <div>
            <h3 class="pub-title">Investigating Vision-Language Model Architectures for 3D Volume Understanding</h3>
            <p class="pub-authors"><strong>Che Liu</strong>, et al.</p>
            <p class="pub-meta">ACL 2025 Findings</p>
            <p class="pub-desc">Architectures for medical 3D volume comprehension with vision-language models.</p>
          </div>
        </div>
        <div class="pub-card">
          <img class="pub-thumb" src="assets/img/profile.png" alt="Multimodal Time-Series">
          <div>
            <h3 class="pub-title">Multimodal Time-Series Learning with Test-Time Enhancement</h3>
            <p class="pub-authors"><strong>Che Liu</strong>, et al.</p>
            <p class="pub-meta">ICML 2024</p>
            <p class="pub-desc">Test-time boosted multimodal time-series models.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="experience">
    <div class="container">
      <h2>Experiences</h2>
      <table class="exp-table">
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>X-humanoid</b>, Remote<br>
            Visiting Researcher (Embodied VLM) — Apr 2025 to Present<br>
            Leading VLM post-training (SFT + self-evolving RL) on large-scale video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/humanoid.png" alt="X-humanoid logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>StepFun</b>, Remote<br>
            Visiting Researcher (OmniLLM) — May 2025 to Present<br>
            Audio-visual-language SFT/RL and omni-capability benchmarks.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/stepfun.png" alt="StepFun logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>DAMO Academy</b>, Beijing<br>
            Research Intern — Nov 2024 to Apr 2025<br>
            Unified multimodal vision pretraining across 2D/3D/video.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/damo.jpg" alt="DAMO Academy logo">
          </td>
        </tr>
        <tr class="exp-row">
          <td class="exp-cell" width="90%">
            <b>AstraZeneca</b>, Cambridge, UK<br>
            Research Intern (Vision-Language Models) — Jul 2024 to Sep 2024<br>
            Synthetic-data VLP; showed synthetic pretraining can beat real-data baselines.
          </td>
          <td class="logo-cell">
            <img class="logo-img-small" src="works/astrazeneca.png" alt="AstraZeneca logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <div class="divider"></div>

  <section class="section" id="education">
    <div class="container">
      <h2>Education</h2>
      <table class="edu-table">
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Imperial College London</b>, UK<br>
            Ph.D. in Multimodal Learning — Feb 2022 to 2026 (expected)<br>
            Supervisors: Rossella Arcucci, Wenjia Bai.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/imperial.png" alt="Imperial College London logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Technical University of Munich</b>, Germany<br>
            Visiting Ph.D. — Apr 2024 to Jun 2024<br>
            Supervisor: Daniel Rückert.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/tum.png" alt="Technical University of Munich logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Swansea University</b>, UK<br>
            M.Sc. (Distinction) in Computational Mechanics — Sep 2019 to Sep 2021.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/swansea.png" alt="Swansea University logo">
          </td>
        </tr>
        <tr class="edu-row">
          <td class="edu-cell" width="90%">
            <b>Shanghai University of Engineering and Science</b>, China<br>
            B.Sc. in Automotive Engineering — Sep 2016 to Jul 2018.
          </td>
          <td class="logo-cell">
            <img class="logo-img" src="edu/sues.png" alt="Shanghai University of Engineering and Science logo">
          </td>
        </tr>
      </table>
    </div>
  </section>

  <footer class="footer" id="contact">
    <div class="container">
      <div>
        <a href="mailto:che.liu21@imperial.ac.uk">Email</a> ·
        <a href="https://github.com/cheliu-computation" target="_blank" rel="noopener">GitHub</a> ·
        <a href="https://scholar.google.com/citations?hl=zh-CN&user=HED_458AAAAJ&view_op=list_works&sortby=pubdate" target="_blank" rel="noopener">Google Scholar</a>
      </div>
      <div style="margin-top: 8px;">
        © 2025 Che Liu. Last updated: Dec 19, 2025.
      </div>
    </div>
  </footer>
</body>
</html>

